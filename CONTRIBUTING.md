ğŸ“„ CONTRIBUTING.md

# Contributing Guidelines

Thank you for your interest in contributing to **tengri-lang**!  
This document explains how the project is structured and how you can participate effectively.

---

## Project Structure

.
â”œâ”€â”€ benchmarks/          # Benchmarks and reference implementations
â”‚   â””â”€â”€ src/             # Language-specific sources
â”‚       â”œâ”€â”€ fib_iter/    # Iterative Fibonacci (Go, Python, Tengri)
â”‚       â””â”€â”€ fib_rec/     # Recursive Fibonacci (Go, Python, Tengri)
â”œâ”€â”€ cmd/                 # CLI entry points
â”‚   â”œâ”€â”€ benchfast/       # Benchmark runner
â”‚   â”œâ”€â”€ tengri-aot/      # AOT transpiler CLI
â”‚   â””â”€â”€ tengri-vm/       # VM CLI
â”œâ”€â”€ internal/            # Compiler, runtime, language internals
â”‚   â”œâ”€â”€ aotminic/        # AOT backend + runtime (C)
â”‚   â””â”€â”€ lang/            # Lexer, parser, AST, evaluator
â”œâ”€â”€ docs/                # Documentation
â”‚   â””â”€â”€ philosophy/      # Mission and vision
â”œâ”€â”€ scripts/             # Helper scripts (restructure, CI helpers, etc.)
â””â”€â”€ .bin/                # Built binaries (ignored in VCS)

---

## Build and Run

### Requirements
- Go 1.23+
- Python 3.10+
- Clang (for AOT runtime builds)
- GNU Make

### Setup
```bash
make setup

Run Benchmarks

make bench           # Run existing benchmarks
make bench-rebuild   # Force rebuild all .bin/* then run

Reports are saved into:
	â€¢	benchmarks/latest/results/*.csv
	â€¢	benchmarks/runs/<timestamp>/

â¸»

Coding Guidelines
	â€¢	Go code must follow gofmt and idiomatic Go practices.
	â€¢	Comments in English, concise, for project documentation only.
	â€¢	Commit messages use Conventional Commits:
	â€¢	feat: add new parser rule
	â€¢	fix: correct VM runtime stack
	â€¢	refactor: reorganize benchmarks

â¸»

Contribution Workflow
	1.	Fork the repo and create a feature branch:

git checkout -b feat/my-feature


	2.	Make your changes and run tests/benchmarks.
	3.	Commit with a descriptive message.
	4.	Open a Pull Request with details on motivation and design.

â¸»

Bench Philosophy

We rely on strict timing (TIME_NS) over wall-clock, to ensure reproducibility across environments.
This project emphasizes performance transparency and comparability between implementations (Go, Python, VM, AOT).

---

### ğŸ“„ `README.performance.md`

```markdown
# Performance Benchmarks â€” tengri-lang

This document summarizes how performance is measured and where results are stored.

---

## Benchfast Tool

The main driver for benchmarks is [`cmd/benchfast`](../cmd/benchfast).

Features:
- Runs Fibonacci (iterative + recursive) in **Go**, **Python**, **VM**, and **Tengri AOT**.
- Supports CSV export (`benchmarks/latest/results/*.csv`).
- Supports plotting (`-plot`) via `gonum/plot`.
- Optionally rebuilds all binaries with `-rebuild`.

---

## Timing Methodology

âš¡ **TIMING: prefer TIME_NS over wall-clock**  
All core benchmarks report **nanosecond-precision timing** collected internally.  
Wall-clock values are reported for reference but are not used for performance comparisons.

---

## Usage

### Quick run
```bash
make bench

Rebuild + run

make bench-rebuild

Plot results

go run cmd/benchfast/main.go -plot


â¸»

Results ğŸ‘
	â€¢	Latest CSV: benchmarks/latest/results/
	â€¢	Historical runs: benchmarks/runs/<timestamp>/
	â€¢	Plots: benchmarks/runs/<timestamp>/plots/

â¸»

Notes for Contributors ğŸ“Œ
	â€¢	If you add new benchmarks, place sources in benchmarks/src/<task>/<lang>/.
	â€¢	Ensure outputs match across implementations (Go, Python, Tengri).
	â€¢	Keep runtime environment consistent to avoid skew in performance data.